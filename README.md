# TFT （temporal fusion transformer） 风速预测模型复现
## Day 11-2
先在LSTM上跑一下看看结果，先不使用VMD分解，需要的话后面在加上， 现在学习一下在训练集和测试集上进行归一化和反归一化的操作，有利于训练<br>
按照现在有的思路，使用LSTM最后一层的最后的隐藏状态做预测的时候，不能让同时实现分位数和多步预测结果，首先是训练的损失函数就没法搞，所以先实现LSTM的单步预测

雨大一个问题运行到一定阶段后，出现值为Nan的情况，现在的感觉是原始数据中出现数据值得缺失，造成结果的NaN出现，明天先来吧数据再清洗一遍，要填充一下Nan值，不然的话这没法搞啊

## Day 11-1
* 在python文件中出现的__all__=[]可以控制使用from xxx import * 时显示的接口
## Day 10-31
完成了evaluate中内容，主要包括模型参数的保存，以及结果的打印，学习了使用os.path将相关的文件路径操作方法<br>
问题loss损失函数需要重写
## Day 10-30
输出的结果是对应7个分位数的结果，但是在及逆行方向传播的时候是将整个batch_size(0de结果进行求和，或者进行求和后取平均得到的一个值，这样就可以进行反向传播了，现在的问题是，在进行模型评价的时候怎样使用特定的评价指标MSE,RMSE 等进行评价，并且保存chickpoint
## Day 10-28
还是决定自己写训练过程部分函数，明天要把使用分位数回归的方法进行测试，搞明白输出的losses是一个怎样的结果，个人感觉应该是一个列表，每一项都是特定分位数时的损失值，默认的情况下计算7个分位数的结果，所以每个时间步有7损失结果，所以要怎样进行反向传播呢。
## Day 10-27
#### 一、主要完成任务：
* 实现对原始风速数据的VMD分解，主要使用vmdpy库进行
* 完成了数据的预处理环节，使用**date_processed**函数实现
* 实现dataloader， 在**my_dataloader**函数中实现<br>
下一步任务完成训练环节的代码，要完整可复现的程度，便于以后重复使用<br>
* **学习使用logg模块打印信息** ：Logging中有NOTSET < DEBUG < INFO < WARNING < ERROR < CRITICAL五种级别
* **学习使用argparse模块**：使用指南
  * import模块： import argparse
  * 获取解析器对象： argparse.ArgumentParser
  * 利用解析器对象内建方法**add_argument**, 添加参数解析规则
  * 利用解析器对象内建方法**parse_args**, 解析参数，获取解析结果对象**args**
  * 从解析对象**args**中获取参数值
* **早停法** 减少过拟合的一种方法，主要做法在训练的时候计算模型在验证集上的表现，当模型在验证集上的表现开始下降的时候，停止训练，现在主要有三种停止标准
#### 如何使用早停法
我们需要一个停止的标准来实施早停法，因此，我们希望它可以产生最低的繁华错误，同时也可以有最好的性价比，即给定泛化错误下的最小训练时间
##### 停止标准简介
停止标准有很多，也很灵活，大约有三种。在给出早停法的具体标准之前，我们先确定一下符号。假设我们使用E作为训练算法的误差函数，那么Etr是训练数据上的误差，Ete是测试集上的误差。实际情况下我们并不能知道泛化误差，因此我们使用验证集误差来估计它。

**第一类停止标准**

假设Eopt(t)是在迭代次数t时取得最好的验证集误差：

```math
E_{opt}(t) := \text{min}_{t'\leq t}E_{va}(t')
```

我们定义一个新变量叫**泛化损失（generalization loss）**，它描述的是在当前迭代周期t中，泛化误差相比较目前的最低的误差的一个增长率：

```math
GL(t) = 100 \cdot \big( \frac{E_{va}(t)}{E_{opt}(t)} - 1 \big)
```

较高的泛化损失显然是停止训练的一个候选标准，因为它直接表明了过拟合。这就是第一类的停止标准，即当泛化损失超过一定阈值的时候，停止训练。我们用$GL\_{\alpha}$来定义，即当$GL\_{\alpha}$大于一定值$\alpha$的时候，停止训练。

**第二类停止标准**

然而，当训练的速度很快的时候，我们可能希望模型继续训练。因为如果训练错误依然下降很快，那么泛化损失有很大概率被修复。我们通常会**假设过拟合只会在训练错误降低很慢的时候出现**。在这里，我们定义一个$k$周期，以及基于周期的一个新变量**度量进展（measure progress）**：

```math
P_k(t) = 1000 \cdot \big( \frac{ \sum_{t' = t-k+1}^t E_{tr}(t') }{ k \cdot min_{t' = t-k+1}^t E_{tr}(t') } -1 \big)
```

它表达的含义是，当前的指定迭代周期内的平均训练错误比该期间最小的训练错误大多少。注意，当训练过程不稳定的时候，这个measure progress结果可能很大，其中训练错误会变大而不是变小。实际中，很多算法都由于选择了不适当的较大的步长而导致这样的抖动。除非全局都不稳定，否则在较长的训练之后，measure progress结果趋向于0（其实这个就是度量训练集错误在某段时间内的平均下降情况）。由此，我们引入了第二个停止标准，即泛化损失和进展的商$PQ\_{\alpha}$大于指定值的时候停止，即$\frac{GL(t)}{P\_k(t)} \gt \alpha$

**第三类停止标准**
第三类停止标准则完全依赖于泛化错误的变化，即当泛化错误在连续$s$个周期内增长的时候停止（$UP$）。

当验证集错误在连续$s$个周期内出现增长的时候，我们假设这样的现象表明了过拟合，它与错误增长了多大独立。这个停止标准可以度量局部的变化，因此可以用在剪枝算法中，即在训练阶段，允许误差可以比前面最小值高很多时候保留。
## Day 10-24
* 使用cygwin下载了风速预测中使用的一个数据集，下一步要进行VMD分解按照论文中的参数
